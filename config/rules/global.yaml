ruleset_version: "1.1"
engine_min_version: ">=2.8.0"
scope: global
metadata:
  description: "Global rules that apply to all LLM interactions"
  created_by: "system"
  created_at: "2025-01-20T00:00:00Z"
rules:
  - name: "rate_limit_check"
    scope: global
    priority: 90
    action: deny
    description: "Prevent excessive API usage"
    enabled: true
    conditions:
      request_count_per_minute: "request_count_per_minute > 100"
    parameters:
      error_message: "Rate limit exceeded. Please wait before making more requests."
      retry_after_seconds: 60

  - name: "content_safety_check"
    scope: global
    priority: 95
    action: deny
    description: "Block harmful or inappropriate content"
    enabled: true
    conditions:
      content_safety: "contains_harmful_content == true"
    parameters:
      error_message: "Content violates safety guidelines."
      categories: ["harassment", "hate", "violence", "adult"]

  - name: "model_availability_check"
    scope: global
    priority: 80
    action: modify
    description: "Ensure requested model is available"
    enabled: true
    conditions:
      model_status: "model_name not in available_models"
    parameters:
      fallback_model: "gpt-3.5-turbo"
      warning_message: "Requested model unavailable, using fallback"

  - name: "token_limit_warning"
    scope: global
    priority: 50
    action: warn
    description: "Warn when approaching token limits"
    enabled: true
    conditions:
      token_usage: "prompt_length > 3000"
    parameters:
      warning_message: "Large prompt detected. Consider breaking into smaller chunks."
      max_recommended_tokens: 4000

  - name: "default_allow"
    scope: global
    priority: 1
    action: allow
    description: "Default allow rule for requests that don't match other rules"
    enabled: true
    conditions: {}
    parameters:
      message: "Request approved by default policy"